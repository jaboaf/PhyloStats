using StatsBase
using GRUtils
#=
C_L(s::String) = [count(==(l),s) for l in ['A','C','G','T']];
function C_N(itr)
    M = maximum(itr)
    cnts = zeros(Int,M)
   	for v in itr cnts[v] += 1 end
    return cnts
end

heC_N(values(countmap(C_L.(SEQs))))
C_N(values(countmap(sort.(C_L.(SEQs)))))
=#
# A large amount of Genomic and Genetic data is widely available. An observation typically consists of a sequence of nucleotide bases, and any information associated with it; for example, specie, time, or place. We call this a genomic observation if the sequence is a full length DNA sequence, and a genetic one if the sequence corresponds to a gene identified by a sub-sequence of DNA. The structure of sequence data is quite simple: pick any number of 'A's,'C's,'G's, and 'T's, and arrange them in any way. Frequently, sequences contain the letter 'N', denoting that the sequencing machines was unable to determine the nucleotide at a position. These machines are also subject to incorrectly reading a nucleotide.

# Here are some references:
# Pfeiffer, F., GrÃ¶ber, C., Blank, M. et al. Systematic evaluation of error rates and causes in short samples in next-generation sequencing. Sci Rep 8, 10950 (2018). https://doi.org/10.1038/s41598-018-29325-6
# Ma, X., Shao, Y., Tian, L. et al. Analysis of error profiles in deep next-generation sequencing data. Genome Biol 20, 50 (2019). https://doi.org/10.1186/s13059-019-1659-6

# Statistical methods should be able to accomodate sequence data alone, and more importantly, genomic or genetic observations. These must be computationally feasible in context of the quantity of data available. Otherwise researchers are forced to use less data than what is available, which leaves the world less informed. This is not good.

#( Its oddly hard to find the number of observations in a bunch of the covid-19 research, and many of the papers sample from their data).

# \section{Data}
# We've been data-less, so for concreteness I've been playing with covid data.  This was retrieved from, and publicly available at the National Center for Biotechnology Innovation (NCBI).

# I have a hard copy of all of the "complete" sequences as of ~ 2 weeks ago on my pc (there are ~ M = 356,471 of these), and i put a "toy" sample of start of covid to 5/31/20 in the github to try different stuff out with. Generally the following variables are available.
# \begin{itemize}\item Accesion: This seems to uniquely identify a sequence
# \item Submitters: This is a list of names of the people attached to the submission of the sequence.
# \item Release Date: This is the date the submission was released I think.
# \item Pangolin: unsure.
# \item Species: This takes the value "Severe acute respiratory syndrome-related coronavirus"
# \item Molecule Type: This takes the values "ssRNA(+)"
# \item Length: Some Integer. I am wondering why these are not all multiples of 3.
# \item Geo Location: Of the form "Country:City"
# \item USA: If given, this is the state of the sample.
# \item Host: This takes the values "Homo sapiens", "Felis Catus", and "Canis Lupus Familiaris"
# \item Isolation Source: Where/how the sample was collected from the source, e.g. swab.
# \item Collection Date: Date of collection, in any of the following formats, YYYY, YYYY-MM, or YYYY-MM-DD
#\item Sequence: This is some word on the alphabet: 'A','C','G','T','N'. 'N' denotes "unknown nucleic acid residue". There seem to be quite a bit of 'N's. Approximately, 10\% of the first sequence is 'N'.
#\end{itemize}

# In the toy example, I have chosen the following variables based on my interests: collection date, location, sequence. I also used length, but this isn't a variable, it is statistic (right?).

# \section{Information Space}
# I'd like to break this section up into a couple parts.
# 1) Preliminary Definitions and Notations:
# 2) Sequence Space: This accomodates any observed sequence with no 'N's.
# 3) Ambigious Sequence Space: This accomodates any observed sequence.
# 4) Observation Space: This combines ambiious sequence space with other information.
# 5) Data Space: I don't know yet, this may be better as its own section.

#\section{Preliminary Definitions and Notations}
B = Set(['A','C','G','T'])
B\bar = ['A';'C';'G';'T'] # mathematically this is equal to a 4-tuple
data = read("some_sequences.fasta",String);
n = count(==('>'),data)
yyyys = String[]; mms=String[]; dds=String[];
ctrys=String[]; lens= Int[]; seqs = String[];
SAMPLE = Tuple{String,String,String,String,Int64,String}[]
for x in split(data,'>')[2:end]
	x = split(strip(x),'|') # gives date,location,length,seq
	
	ctry = String(split(x[3],':')); push!(ctrys,ctry)
	len = parse(Int,x[2]); push!(lens,len)
	seq = String(x[5]); push!(seqs,seq)

	k = length(split(x[1],"-"))
	if k ==0 yyyy = ""; mm = ""; dd = ""
	elseif k==1 yyyy=split(x[1],"-")[1]; mm= ""; dd = ""
	elseif k==2 yyyy,mm = String.(split(x[1],"-")); dd = ""
	else yyyy,mm,dd = String.(split(x[1],"-"))
	end
	push!(yyyys, String(yyyy)); push!(mms, String(mm)); push!(dds, String(dd));

	push!(SAMPLE, (yyyy,mm,dd,ctry,len,seq))
end; # note that the first element of split(data,'>') is ""
sort!(SAMPLE);
YYYY = sort(unique(yyyys)); MM = sort(unique(mms)); DD = sort(unique(dds));
CTRY=sort(unique(ctrys)); LEN =sort(unique(lens)); SEQ = sort(unique(seqs));
println("These are $(length(CTRY)) unique countries ")
println("These are $(length(LEN)) unique lengths of sequences ")
println("These are $(length(SEQ)) unique sequences")

# \section{Sequence Space}

# Lets start by identifying the measurable space that each (completely specified) sequence is contained in because then our sample will be in the n-fold product of this space (where n in the number of observations in our sample).
# So, we are in need of space that accomodates all of our observations. There are a couple of weird things that come up in biological data that we should (be able to) deal with such as 'N's in sequences denoting that the nucleotide in a position was not able to be observed. Other things include alignment (though I am not sure if this is a scientific procedure, under the biologists juristiction, or something of a statistical nature). These are not reflective of the biological information, rather, the (erronous or possibly informed)data-generating process. I am not a biologist or geneticist, so I will ignore the question of alignment until later. On the other hand, I do feel well equipped to handle 'N's because they are simple: each instance of 'N' is a placeholder for a nucleotide base. Also, it seems important to include them in our understanding of the data because they show up quite a bit.

numberOfSequencesWithNs = count(s ->'N' in s, seqs)
println(" $numberOfSequencesWithNs of the $n sequences contain at least one N, which is $(numberOfSequencesWithNs/n) percent of the sequences")

# Upon looking into the Ns I found out that there are other letters that occur in the sequences: 
print(union(unique.(seqs)...))

M_N(s::String) = count(==('N'),s)

numNsInseqs = M_N.(seqs);

#= https://www.qmul.ac.uk/sbcs/iubmb/misc/naseq.html
3.1. Guanine. adenine, thymine, cytosine: G,A,T,C
3.2. Purine (adenine or guanine): R
3.3. Pyrimidine (thymine or cytosine): Y
3.4. Adenine or thymine: W
3.5. Guanine or cytosine: S
3.6. Adenine or cytosine: M
3.7. Guanine or thymine: K
3.8. Adenine or thymine or cytosine: H
3.9. Guanine or cytosine or thymine: B
3.10. Guanine or adenine or cytosine: V
3.11. Guanine or adenine or thymine: D
3.12. Guanine or adenine or thymine or cytosine: N
so we need a map
'A'=>'A'
'B'=>['C','G','T']
'C'=>'C'
'D'=>['A','G','T']
'E'=>???
'F'=>???
'G'=>'G'
'H'=>['A','C','T']
'I'=>???
'J'=>???
'K'=>['G','T']
'L'=>???
'M'=>['A','C']
'N'=>['A','C','G','T']
'O'=>???
'P'=>???
'R'=>???
'S'=>['G','S']
'T'=>'T'
'U'=>???
'V'=>['A','C','G']
'W'=>['A','T']
'X'=>???
'Y'=>['C','T']
'Z'=>???
=#


# It appears that the length of a sequence and the counts of nucleotides in a sequence vary across the sample. So our sequence space should accomodate sequences with different lengths and different counts of nucleotides. 

# Without further chattiness, denote the set of nucleotide bases by B. i.e. B = \{'A','C','G','T'\}.
# That means the set of sequences of length l is B\^l = \{ (w\^1,\dots,w\^l) \mid 1\lei\lem, w\^i \in B \}.
# So, the set of all sequences is the union over l \in \mathbb{N} of sequences of length l i.e. \bigcup_{l\in\mathbb{N}} B\^l. We will denote this set B^\star.

# Bada bing! Any sequence of 'A','C','G','T' is an element of B^\star, so it seems like B^\star is a good candidate for the set of possible outcomes for an observation. We can take \mathscr{P}(B^\star) to be the sigma-algebra. We could endow this space with a measure, and if we were being honest bayesians we'd do this before looking at the data.
# So, what probability measure would you like?
# It is easy to pick ~a~ probability measure on this discrete space, however I don't enjoy doing this because I know very very little about covid so I'm just making arbitrary choices. Also, I find it way more fun to explore data, than to arbitrarily specify some probability measure that ought to reflect how shit actually works in the world.

# So maybe we could take a more adventurous approach that allows us to perform inference (whatever that means) and estimate probabilities in a consistent manner (in the precise logical definition). Any probability measure that reflects the data generating process seems to satisfy these desires.
# Recall that we are still in the world of "point data", \mathbf{x} = (x_1,\dots,x_n) where x_i \in B^\star
# One such measure, is the empirical measure: \mathbb{P}\_n = 1/n \sum_i \delta_{x_i}
# \mathbb{P}\_n:B^\star\to[0,1]
# In the "non-point data" world we will have to be more careful because \int_{B^\star}\delta_{X}(w) dw \ge 1 for X \in \mathscr{P}(B^\star)\\varnothing. We will return to this.

# We have not explitly defined a parameter space or a parametric family of functions to define the empirical measure \mathbb{P}\_n, however we can do this using "parameters". Below are some ways of doing this:
# 0.0) Let $ \Omega = B^\star $. A density (or function) on $ B^\star $ is $ f:B^\star\to\mathbb{R} $, so $ f = \sum_{w \in B^\star} f(w)1_w = \sum_{w \in B^\star} f_w 1_w $. Hence, the collection of densities is a "parametric family" with the parameter (f_w)_{w \in B^\star}. NOTE: Technially, for $ (f_w)_{w\in B^\star} $ to be well defined we must (be able to) order $ B^\star $; one can do this by partially ordering $ B^\star $ by length and then lexicographically w.r.t. $ \bar{B}$. The order of application of these partial orders gives a total order, $<$, and $(B^\star,<)$ looks like: $ ()<(A)<(C)<(G)<(T)<(A,A)<(A,C)<\dots<(T,G)<(T,T)<(A,A,A)<(A,A,C)<\dots<(T,T,G)<(T,T,T)\dots $. One could also partially order $ B^\star $ lexicographically w.r.t. \bar{B} and then by length, so $(B^\star,<') $ would look like: $ ()<'(A)<'(A,A)<'(A,A,A)<'\dots <'(A,T)<'\dots <'(B)<(B,B) $
# 0.1) A probability density on $ B^\star $ is a density $ f:B^\star\to\mathbb{R} $ such that $ \sum_w |f_w| = 1 $. Hence, we may have "parametric family" of probability densities with the parameter $(f_w)_{w\inB^\star}$.
# 0.2) A probability distribution on $ B^\star $ is a density $ f:B^\star\to\mathbb{R} $ taking non-negative values, i.e. $ f_w \ge 0 $, such that $ \sum_w f_w = 1 $. Hence, we may have "parametric family" of probability distributions with the parameter $(f_w)_{w\inB^\star}$.
# 1) Let $\Theta = \mathbb{N}\^4$ be the parameter space. Each $ \theta \in \Theta $ determines an equivalence class of $ B^\star $ where there are $ \theta^1=\theta_A $ 'A's, $ \theta^2=\theta_C $ 'C's,$ \theta^3=\theta_G $ 'G's, and $ \theta^4=\theta_T $ 'T's. The equivalence class is the set of all rearrangements of $ (A)^{\otimes\theta_A}\otimes(C)^{\otimes\theta_C}\otimes(G)^{\otimes\theta_G}\otimes(T)^{\otimes\theta_T} = \bar{B}^{\theta_{\bar{B}}} $ is $ \mathfrak{S}^{|\theta|} \bar{B}^{\theta_{\bar{B}}} $. And $\Theta $ partitions $ B^\star $ because for any $ w \in B^\star , M_{\bar{B}}w \in \mathbb{N}^4 $ means that $ M_{\bar{B}}w = \vartheta $ for some $ \vartheta\in\Theta $.
# Note: You can replace $(b)^{\otimes\theta_b} $ with $ {b}^{\times\theta_b}$  to get an equivalent expression. The latter is a more set theorhetic approcach. I will avoid this for a very particular reason that I hope to get to later.
# 2) Let $ \Phi = \{ \phi\in\mathbb{N}\^4 \mid 1 \le i \le j \le 4 \implies\phi\_i\le\phi\_j\} $. Each $ \phi \in \Phi $ determines a equivalence class which is a union of equivalence classes from 1. Namely, the set of w in $ B^\star $ such that w has $\phi_1 b\_1s, \phi_2 b\_2s, \phi_3 b\_3s$, and $\phi_4 b\_4s$, and ${b\_1,b\_2,b\_3,b\_4}=B$. Namely, $ \phi\in\Phi $ determines the equivalence class, $ \bigcup_{g\in\mathfrak{S}^4} \mathfrak{S}^{|\phi|} \bar{B}^{g \phi} $. This partitions $ B^\star $ because it is a coarsening of the partiion given in 1. Also, $ \bigcup_{\phi\in\Phi} \mathfrak{S}\phi = \mathbb{N}\^4 $.
# 2.1) This equivalence class may be given in another way. Let $\widehat{\mathfrak{S}_B}:B^\star\to B^\star$ be given by $\bigcup_l \bigcup_{g\in\mathfrak{S}_B}g^{\timesl}$. This is the action of the union over l of the diagonal subgroups of  $\mathfrak{S}_B^{\times l}$ acting on the union of their domains $B^l$. Then the equivalence class given by $\phi$ from (2),  $\bigcup_{g\in\mathfrak{S}_B^4} \mathfrak{S}_B^{|\phi|} \bar{B}^{g\phi}$, is the same thing as $\mathfrak{S}^{|\phi|}\widehat{\mathfrak{S}_B}\bar{B}^\phi$. I love this one for a lot of reasons: $\mathfrak{S}^{|\phi|}$ and $\widehat{\mathfrak{S}_B}$ commute, the klien 4 group is a subgroup of $ \widehat{\mathfrak{S}_B}$ if you're willing to associate $\mathbb{Z}_2^2$ with nucleotide bases (i have a very very fun, perhaps computationally useful, idea here with $\mathbb{C}$), every subgroup of $\widehat{\mathfrak{S}_B}$ is  isomorphic to a torus of some dimension. There are more reasons.... 
# 3) Let $\psi = (\psi^{(l)})_{l\in\mathbb{N}}$ where $\psi^{(l)} \inB^l$. Then for any $w \in B^l$ we may find an element of $ \mathfrak{S}_B^l $, such that $ g\psi^{(l)} = w $. There may be multiple such g, so we can actually find a subgroup H of $ \mathfrak{S}_B^l $, such that $ H\psi^{(l)} = \{w\} $.
# 4) This one is a riff on 3) and the finite length sequence approach to everything. First, let $ \Psi \inB^\infty $ be some fixed element. Now, for the rest of this bullet, let w \in B^l. Instead of thinking of w as some finite sequence, we could think of it as an equivlence class of sequences $ \tilde{w} \subset B^\infty $, where every $ v \in \tilde{w} $ has its first $ \# w $  elements equal to w, so $\tilde{w}$ is an element of $ B^\infty / \langlee^{(1,\dots,\# v)} v' = v \mid v\inB^\star\rangle=\tilde{B^\star} $. Formally, $\tilde{}$ is a function, $\tilde:B^\star\to\mathscr{P}(B^\infty)$, which is defined by $ w\mapsto\{v\inB^\infty\mid (v^1,\dots,v^{\# w}) = w \}$. Since, $B^\infty = \mathfrak{S}_B^\infty \Psi $, there is a subgroup $ H = (H_1,H_2,\dots) \in \mathfrak{S}_B^\infty $ such that $ H\Psi = \tilde{w}$. The structure of H is fairly simple: $ h \in H_1 $ maps $ \Psi^1 $ to $ w^1,\dots,h \in H_{\# w}$ maps $ \Psi^{\# w} $ to $ w^{\# w},h\inH_{\# w + 1} $ maps $ \Psi^{\# w +1}$ to any element of B,\dots. So for $ 1 \le i \le \# w, H\_i = (\Psi^i \: w^i)\mathfrak{S}_{ B / \{\Psi^i,w^i\}}I_B $, and for $ i>\#w, H\_i = \mathfrak{S}_B $.

# Some topic-specific thoughts on each parameterization before we get concrete.
# 0) This basic probability on the combinitorial structure of sequences. I would avoid using this as THE approach. This considers the dual space of B^\star, aka, the space of linear functionals from B^\star into a field of choice. After looking at it this way, the dual space gives a better perspective on measures, and is the same as (V_B^\star)^* when \{e_A,e_C,e_G,e_T\} is an orthonormal basis for V_B. If \{E_A,E_C,E_G,E_T\} is orthogonal (so ||E_b|| need not be 1), then we can consider densities over the basis E_w = ||E_w|| e_w; these elements can get very big or very small, very slowly or very quickly.
# 1) This one is probably the most intuitive and very useful. For any sequence its not so hard to count the number of 'A's,'C's,'G's, and 'T's.
function M_B\bar(s::String)
	cnts = zeros(Int,4)
	for c in s
		cnts += B\bar .== c
	end
	return cnts
end

\mathbb{A} = M_B\bar.(seqs);
sum.(\mathbb{A})
# note !! n on x axis,
plot(sort(sum.(\mathbb{A})))
#e.g. plot(sort(sum.(\mathbb{A}))[1:10000])
maximum(\mathbb{A})

#=
Polya enumeration.
Y^X is set of functions from X to Y
|\mathfrak{S}||Y^X/\mathfrak{S}| = \sum_{g\in\mathfrak{S}} |Y|^{c(g)}

Burnsides lemma
X^g = {x\inX|gx=x}
|\mathfrak{S}||X/\mathfrak{S}| = \sum_{g\in\mathfrak{S}} |X^g|

\mathfrak{S}_x = {g\in\mathfrak{S}| gx=x}
|\mathfrak{S}x| = [\mathfrak{S}:\mathfrak{S}_x] = |\mathfrak{S}|/|\mathfrak{S}_x| 
=#

# \section{On Models}
# Acccording to Se Yoon Lee, Bayesian models are given by \{P(y|\theta),\pi(\theta)\}. This is supposed to be read as \{P(\cdot|\theta):\Omega\to\mathbb{R}\mid \theta \in \Theta\}\cup\{\pi:\Theta\to\mathbb{R}\} and interpreted as the collection of conditional probability densities and a prior.
# Se Yoon Lee, Gibbs sampler and coordinate ascent variational inference: A set-theoretical review, https://arxiv.org/pdf/2008.01006.pdf

# According to Sullivant, a parametric statistical model given by a parameter spcae with a family of conditional probability densities, (\Theta,\{P_\theta:\Omega\to\mathbb{R}\mid\theta\in\Theta\}).

# In either case, \Omega is called the outcome space.

# Lets get one thing out of the way:
# \[ P_\theta(\cdot) = P(\cdot|\theta) \]

# Can we all just get along and dance?:
# P_\theta(\cdot) = "Traditional Statistics" = "Left \theta-ists"
# P(\cdot|\theta) = "Bayesian Statistics" = "Right \theta-ists" 
# Using, P_\theta(\cdot) = P(\cdot|\theta), we arrive at the following:
# \[ "Left \theta statistics" = "Traditional Statistics" = P_\theta(\cdot) = P(\cdot|\theta) = "Bayesian" = "Right \theta statistics" \]

# Why can't we define a model using \mathscr{M}:\Omega\times\Theta\to\mathbb{R}_+? I am not sure, lets see what happens.
# Suppose we define a model by a function \mathscr{M}:\Omega\times\Theta\to\mathbb{R}_+,
# A conditional density given by \theta is P_\theta(\cdot)=P(\cdot|\theta)=\int_\Omega \mathscr{M}(\cdot,\theta)d\omega = \frac{\partial\mathscr{M}(\cdot,\vartheta)}{\partial\vartheta}|_{\vartheta=\theta}
# The family of conditional probability densities is \{\int_\Omega \mathscr{M}(\cdot,\theta)d\omega \mid \theta\in\Theta\}
# The prior density is \pi(\cdot) = \int_\Omega \mathscr{M}(\omega,\cdot) d\omega

# The "evidence" of the model is m(\cdot) = \int_\Theta P(\cdot|\theta)\pi(\theta)d\theta, huh, \int_\Theta P(\cdot|\theta)\pi(\theta)d\theta = \int_\Theta \int_\Omega \mathscr{M}(\cdot,\theta)d\omega \int_\Omega \mathscr{M}(\omega,\theta) d\omega d\theta = \int_\Theta \int_\Omega \int_\Omega \mathscr{M}(\cdot,\theta)\mathscr{M}(\omega,\theta)d\omega d\omega d\theta.

# Maybe consider, m(\cdot) = "\int_\Theta P(\cdot|\theta)\pi(\theta)d\theta" = "\int_\Theta P(\cdot,\theta) d\theta" = \int_\Theta\mathscr{M}(\cdot,\theta)d\theta. This may be easily intereted as a probability density because it is a function from \Omega to \mathbb{R}_+. Lets take a moment to reflect, we simply specified \mathscr{M} as a non-negative function over \Omega\times\Theta and it appears as though the common usage of "P" is precisely this arbitrary function. Huh, we never specified anything more about \mathscr{M}, so the number, \int_\Omega\int_\Theta \mathscr{M}(\omega,\theta)d\omegad\theta, could really be any number in \mathbb{R}_+, which includes 0. (As a side note, lets stop using notation that makes the most basic letter for the most basic concept an arbitrary non-negative function.)

# We observed that \int_\Omega\int_\Theta \mathscr{M}(\omega,\theta)d\omegad\theta is a number, and gave names to the following integrals: \int_\Omega \mathscr{M}(\cdot,\theta)d\omega, \int_\Omega \mathscr{M}(\omega,\cdot)d\omega, and \int_\Theta \mathscr{M}(\cdot,\theta) d\theta.

# There is one more integral of \mathscr{M} to consider: \int_\Theta \mathscr{M}(\omega,\cdot)d\theta.
# This is called the "posterior distribution", typically written as "\pi(\theta\mid\omega)", and holds two honors: the most desired thing to compute and the biggest pain in statistics' butt. Lets first observe that we only supposed a parametric statistical model (bayesian or not) was provided by a non-negative function on the product of an (arbitrary) outcome space (\Omega) and a (arbitary) parameter space (\Theta). The statisticain often knows the parameter space. So to determine the posterior, given some observation, the statistician should evaluate the integral.
# Often, statisticians have multiple observations x_1,\dots,x_n. The data consisting of these observations is typically denoted, \mathbf{X}. So if the statistician wishes to compute the "posterior given the data", they may compute it \int_\Theta \mathscr{M}(\mathbf{X},\cdot)d\theta; that is, if and only if \mathbf{X} \in \Omega.
# In many cases, the data consistsing of observations is not in the form of an observation. The form that \mathbf{X} takes is up to the modeller's discretion.
# The data, \mathbf{X}, could be:
# a set S = \{x_1,\dots,x_n\}
# a set function f:S\to\mathbb{N} counting the number of times an element of S occured in the sample, i.e. f(x) = \sum_{i=1}^n 1_{x=x_i}
# or a n-tuple, e.g. (x_1,\dots,x_n), e.g.(x_n,\dots,x_1)
# these are some possibilites when \Omega is a set.
# In the case that \Omega is a set, we could consider the data to be given by any sum or product or really any binary operation, e.g. +,*,\bigoplus,\bigotimes,\vee,\wedge,\circ,[\cdot,\cdot].

# It seems like most of the difficulties encountered in the bayesian notation come from blatant misuse of bayes' rule. If P:\Omega\to[0,1], then the probability of A given B may be computed using the formula for conditional probability:
# \[ P(A\capB) = P(A|B)P(B) \]
# If you apply this formula to $P(A\capB)$ and $P(B\capA)$, you end up with Bayes' rule:
# \[ A\capB=B\capA \implies P(A\capB)=P(B\capA) \implies P(A|B)P(B)=P(B|A)P(A) \implies P(A|B)=\frac{P(B|A)P(A)}{P(B)} \]
# Bayesians are understandably excited to use Bayes' rule. So when tasked with computing the posterior distribution, typically written as "P(\theta|\mathbf{X})", they jump to compute \frac{ P(\mathbf{X}|\theta)P(\theta)}{P(\mathbf{X})}. It is interesting this formula has lasted this long beacause:
#\[ P(\theta|\mathbf{X})= \frac{ P(\mathbf{X}|\theta)P(\theta)}{P(\mathbf{X})} \implies P(\theta|\mathbf{X})= \frac{P(\mathbf{X}\cap\theta)}{P(\mathbf{X})} \]
]  tend to jump to the 
P(A|B) = \frac{P(B|A)P(A)}{P(B)} \]
# This formula comes from the formula 

# I am going to be a bad bayesian. Let P = \mathbb{P} and \pi = \mathbb{pi}. \mathbb{pi} = 1/n\sum_i \delta_{N_\bar{B}(X_i)}
#(\theta|x) = P(x|\theta)\pi(\theta) / m(x)

# e\^i(n_1e_1+\dots+n_ke_l) = n_i
# for some orthogonal (?) e_b \in V_B and E = (e_A,e_C,e_G,e_T), the dual of E is \hat{E} = (e^1,e^2,e^3,e^4).

# E\^TE\hat = I_l = J_l*1_l
# If we let E_w = (e_{w\^1},\dots,e_{w\^l}); 
# maybe let \otimesE_w = e_{(w\^1,\dots,w\^l)} and \otimesE\hat^\mathbb{N} = e^{(1,\dots,l)}
# \otimesE\^T\otimesE\hat = \lambda_{\lel} = \sum_i e_{w^i}
# e.g. E_w = e_


# \section{Kingman's Approach}
# Kingman has an interesting article, "Random Partitions in Population Genetics", in which he defines a partition structure, describes its connection with Ewens' sampling formula, and provides some probablistic tools. In this paper he views a partition as an integer partition of the frequency distribution of the sample:
# Let M_{B^\star}: CtsTime \to \mathbb{N}^{B^\star} be a counting process (using the ordering described in Sequence Space.0), and let \mathbb{M}_{B^\star} be the empirical analouge. Now define another counting process M_N:\mathbb{N}^{B^\star} \to \varpi_N, where \varpi_N = \{a\in\mathbb{N}^N \mid\sum_{j=1}^N ja_j = N\} is the set of ordered integer partitions of N. Kingman's random partition is \pi = M_{|\mathbb{M}_{B^\star}(\text{right now})|}\circ\mathbb{M}_{B^\star}(\text{right now}). If \pi=(a_1,a_2,\dots,a_n), then a_1 sequences appeared once, a_2 sequences appeared twice, ..., a_n sequences appeared n times.
# Kingman then defines the set of all probability distributions on \varpi_n as \Pi_n, so any P \in \Pi_n satisfies P(\pi)\ge0 and \sum_{\pi\in\varpi_n} P(\pi) = 1. So you might wonder what the probability of \pi=(a_1,a_2,\dots,a_n) is. It could be anything. If you also specified that for every m<n (these are variables for sample size), P_m\in\Pi_m must be the distribution arising from sub sampling without replacement from P_n \in \Pi_n, you can consider the probability of \pi :
# "Consider an infinite population divided into a countable number of sub-populations labelled (say) by colours 1,2,3,..., and suppose that the proportion of the population which is of colour i is x_i, where x_i \ge 0, \sum_{i=1}^\infty x_i = 1" (p.4). Then P_n(\pi) = \phi_\pi(y) = n!\prod_{j=1}^n (j!)^{-a_j} \sum_{v\in[0:n]^\infty \mid M_n(v) = \pi \} x_1^{v_1}x_2^{v_2}\dots . For the particular choice of y=x, this is a number, otherwise this is a function of the variable y.

# I don't want to get too deep into this because I don't think this is the right perspective. It seems that the motivation for this view of a partition (and the resulting importance of a partition structure) comes from considering "a geneticist that examines a sample of n gametes from a population of size N, and whose experimental techniques do not permit a labelling of the alleles he can distinguish"(p.3). I challenge you to distinguish things without being able to label them.

hashSEQ = Dict([s=>i for (i,s) in enumerate(SEQ)]);
countingSEQprocess = zeros(Int,(n,length(SEQ)));
for (i,s) in enumerate(seqs)
	countingSEQprocess[i,hashSEQ[s]] += 1
end

# Letters or fancy letters used thus far:
# 'A', B, 'C', 'G', 'T', 'N',
# N for random n,
# M for counting
# ^\star,\mathscr{P},\mathbb{N},\int\Phi,\phi